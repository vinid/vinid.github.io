[{"authors":null,"categories":null,"content":"Hello! I am Federico Bianchi, I am a post-doctoral researcher at Stanford University with Prof. Dan Jurafsky. I was a post-doctoral researcher at Bocconi University (with Prof. Dirk Hovy), Milan, Italy. I did my PhD at University of Milano Bicocca (with Prof. Matteo Palmonari).\nYou can find my work in the main track of different AI venues (e.g., NAACL, EACL, AAAI, ACL, ISWC, RecSys) and Q1 Journals (e.g., Cognitive Science, Nature PJQI, SWJ) and my work as also been featured in press or in company media outlets (see, 1, 2, 3, 4).\nI have also worked/collaborated with companies: I have collaborated as a Data Scientist for Instal LLC and I have an ongoing research collaboration with people at Coveo on e-commerce and recommendation research.\nI have given talks in different places for both Universities (e.g., Edinburgh University, Sussex University, Tilburg University) and Companies (Coveo Labs, LightOn AI, MLOPS Neptune Podcast, Tecton) and tutorials at major AI Conferences (IJCAI, IJCLR).\n","date":1649771300,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1649771300,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello! I am Federico Bianchi, I am a post-doctoral researcher at Stanford University with Prof. Dan Jurafsky. I was a post-doctoral researcher at Bocconi University (with Prof. Dirk Hovy), Milan, Italy.","tags":null,"title":"Federico Bianchi","type":"authors"},{"authors":["Federico Bianchi"],"categories":["Recommender Systems"],"content":"Overview Recommender Systems are probably one of the most popular machine learning systems in production; moreover, recommenders are probably the machine learning products that have the closest contact with the actual users, since they are generally used in an interactive fashion.\nThis is why testing is fundamental in Recommender Systems. Failing to detect low performance in some cases can bring reputational damage to a company.\nRead More\n","date":1660176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660176000,"objectID":"99aab00236fa462fc107e781a6b0fdb5","permalink":"https://federicobianchi.io/post/rounded-evaluation-recommender-systems/","publishdate":"2022-08-11T00:00:00Z","relpermalink":"/post/rounded-evaluation-recommender-systems/","section":"post","summary":"This article describes a novel data and code challenge that is currently running: EvalRS. We decided to organize EvalRS with friends from Coveo, Microsoft, and NVIDIA, to better understand evaluation in Recommender Systems.","tags":["Academic","AI"],"title":"A Rounded Evaluation of Recommender Systems","type":"post"},{"authors":["Federico Bianchi"],"categories":["Topic Models","NLP","Language Models"],"content":"Overview Suppose we have a small set of documents in Portuguese that is not large enough to reliably run standard topic modeling algorithms. However, we have enough English documents in the same domain. With our cross-lingual zero-shot topic model (ZeroShotTM), we can first learn topics on English and then predict topics for Portuguese documents (as long as we use pre-trained representations that account for both English and Portuguese).\nRead More\n","date":1660176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660176000,"objectID":"492afb2be927b0c3573fd7a2b32151a1","permalink":"https://federicobianchi.io/post/ctm-topic-model/","publishdate":"2022-08-11T00:00:00Z","relpermalink":"/post/ctm-topic-model/","section":"post","summary":"In this blog post, I discuss our latest published paper on topic modeling in which we introduce Contextualized Topic Models.","tags":["Academic","AI"],"title":"Contextualized Topic Modeling with Python (EACL2021)","type":"post"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1658235600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658235600,"objectID":"cf70bdf03f1e14d3440f79cce3ee20b2","permalink":"https://federicobianchi.io/talk/talk-twitter-demographer/","publishdate":"2022-07-18T13:00:00Z","relpermalink":"/talk/talk-twitter-demographer/","section":"event","summary":"In this talk we describe our newest package for enriching Twitter data","tags":["Twitter","Language Models","Demographics"],"title":"Talk: Twitter-Demographer","type":"event"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1657026000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657026000,"objectID":"981a93fa80de6206dfdd66cc685a3fba","permalink":"https://federicobianchi.io/talk/talk-fantastic-clips-and-how-to-tune-them/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-fantastic-clips-and-how-to-tune-them/","section":"event","summary":"How to fine-tune Contrastive Language-Image Pre-training models for different usecases","tags":[],"title":"Talk: Fantastic CLIPs and How To Tune Them","type":"event"},{"authors":["Federico Bianchi","Debora Nozza","Dirk Hovy"],"categories":[],"content":"","date":1649771300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649771300,"objectID":"0ffb34cbef866356b1fc44743a8b20b0","permalink":"https://federicobianchi.io/publication/language-invariant-properties-nlp/","publishdate":"2022-04-12T14:48:20+01:00","relpermalink":"/publication/language-invariant-properties-nlp/","section":"publication","summary":"Meaning is context-dependent, but many properties of language (should) remain the same even if we transform the context. For example, sentiment, entailment, or speaker properties should be the same in a translation and original of a text. We introduce language invariant properties: i.e., properties that should not change when we transform text, and how they can be used to quantitatively evaluate the robustness of transformation algorithms. We use translation and paraphrasing as transformation examples, but our findings apply more broadly to any transformation. Our results indicate that many NLP transformations change properties like author characteristics, i.e., make them sound more male. We believe that studying these properties will allow NLP to address both social factors and pragmatic aspects of language. We also release an application suite that can be used to evaluate the invariance of transformation applications.","tags":["NLP","Language Invariant Properties","Meaning"],"title":"Language Invariant Properties in Natural Language Processing","type":"publication"},{"authors":["Debora Nozza","Federico Bianchi","Dirk Hovy","Anne Lauscher"],"categories":[],"content":"","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649721600,"objectID":"6ed3c5c49961c546ddcc35d549d34bd8","permalink":"https://federicobianchi.io/publication/honest-hurtful-language-model-lgbtqia+/","publishdate":"2022-04-12T14:48:20+01:00","relpermalink":"/publication/honest-hurtful-language-model-lgbtqia+/","section":"publication","summary":"Current language technology is ubiquitous and directly influences individuals' lives worldwide. Given the recent trend in AI on training and constantly releasing new and powerful large language models (LLMs), there is a need to assess their biases and potential concrete consequences. While some studies have highlighted the shortcomings of these models, there is only little on the negative impact of LLMs on LGBTQIA+ individuals. In this paper, we investigated a state-of-the-art template-based approach for measuring the harmfulness of English LLMs sentence completion when the subjects belong to the LGBTQIA+ community. Our findings show that, on average, **the most likely LLM-generated completion is an identity attack 13% of the time**. Our results raise serious concerns about the applicability of these models in production environments.","tags":["Hate Speech","BERT","NLP"],"title":"Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals","type":"publication"},{"authors":["Debora Nozza","Federico Bianchi","Dirk Hovy"],"categories":[],"content":"","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649721600,"objectID":"90143143d49a94d3f55e12137879769b","permalink":"https://federicobianchi.io/publication/pipelines-social-bias-testing-language-models/","publishdate":"2022-04-12T14:48:20+01:00","relpermalink":"/publication/pipelines-social-bias-testing-language-models/","section":"publication","summary":"The maturity level of language models is now at a stage in which many companies rely on them to solve various tasks. However, while research has shown how biased and harmful these models are, **systematic ways of integrating social bias tests into development pipelines are still lacking. This short paper suggests how to use these verification techniques in development pipelines.** We take inspiration from software testing and suggest addressing social bias evaluation as software testing. We hope to open a discussion on the best methodologies to handle social bias testing in language models.","tags":["Hate Speech","BERT","NLP","Pipelines"],"title":"Pipelines for Social Bias Testing of Large Language Models","type":"publication"},{"authors":["Federico Bianchi","Debora Nozza","Dirk Hovy"],"categories":[],"content":"","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649721600,"objectID":"331bf3956adf93f600ee8b8e492f396a","permalink":"https://federicobianchi.io/publication/xlmemo-multilingual-emotion-prediction/","publishdate":"2022-04-12T14:48:20+01:00","relpermalink":"/publication/xlmemo-multilingual-emotion-prediction/","section":"publication","summary":"We use Large Multilingual Language Models to create a tool for multilingual emotion prediction","tags":["Sentiment Analysis","Language Models","Emotion Detection","Italian","BERT","NLP","Dataset","Multilingual"],"title":"XLM-EMO: Multilingual Emotion Prediction in Social Media Text","type":"publication"},{"authors":["Patrick John Chia","Jacopo Tagliabue","Federico Bianchi","Chloe He","Brian Ko"],"categories":null,"content":"","date":1646179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646179200,"objectID":"89c8e9b4b30afee110f5fc2a229c6ea5","permalink":"https://federicobianchi.io/publication/reclist/","publishdate":"2022-03-02T00:00:00Z","relpermalink":"/publication/reclist/","section":"publication","summary":"In this paper, we propose RecList, a behavioral-based testing methodology. RecList organizes recommender systems by use case and introduces a general plug-and-play procedure to scale up behavioral testing. We demonstrate its capabilities by analyzing known algorithms and black-box commercial systems, and we release RecList as an open source, extensible package for the community.","tags":["NLP","Recommender Systems","Embeddings","eCommerce"],"title":"Beyond NDCG: behavioral testing of recommender systems with RecList","type":"publication"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1633611600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633611600,"objectID":"749fef9e9f36cd460aa29556ec288875","permalink":"https://federicobianchi.io/talk/talk-how-to-train-your-clip/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-how-to-train-your-clip/","section":"event","summary":"How to train a Contrastive Language-Image Pre-training for the Italian Language","tags":[],"title":"Talk: How to Train Your CLIP","type":"event"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1633266000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633266000,"objectID":"d18e98d594cef8793bd7277fd4a4a3c1","permalink":"https://federicobianchi.io/talk/talk-natural-language-processing-to-better-understand-language-and-vice-versa/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/talk/talk-natural-language-processing-to-better-understand-language-and-vice-versa/","section":"event","summary":"We show how NLP can help us understand language aspects like grounding, but also discuss several cognitively-informed methods that can be directly applied to more practical tasks.","tags":[],"title":"Talk: Natural Language Processing to Better Understand Language – and Vice Versa","type":"event"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1631797200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631797200,"objectID":"befd1ecd4108bd765df01deda7fda6df","permalink":"https://federicobianchi.io/talk/talk-contrastive-language-image-pre-training-for-the-italian-language/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-contrastive-language-image-pre-training-for-the-italian-language/","section":"event","summary":"How to train a Contrastive Language-Image Pre-training for the Italian Language","tags":[],"title":"Talk: Contrastive Language-Image Pre-training for the Italian Language","type":"event"},{"authors":["Federico Bianchi","Giuseppe Attanasio","Raphael Pisoni","Silvia Terragni","Gabriele Sarti","Sri Lakshmi"],"categories":[],"content":"","date":1629590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629590400,"objectID":"998aa0368cc2996b98a9ec761feeb19c","permalink":"https://federicobianchi.io/publication/gap-between-understanding-adoption/","publishdate":"2021-08-22T01:41:26+01:00","relpermalink":"/publication/gap-between-understanding-adoption/","section":"publication","summary":"CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal model that jointly learns representations of images and texts. The model is trained on a massive amount of English data and shows impressive performance on zero-shot classification tasks. Training the same model on a different language is not trivial, since data in other languages might be not enough and the model needs high-quality translations of the texts to guarantee a good performance. In this paper, we present the first CLIP model for the Italian Language (CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image retrieval and zero-shot classification.","tags":["Multimodal","Vision","NLP"],"title":"Contrastive Language-Image Pre-training for the Italian Language","type":"publication"},{"authors":["Federico Bianchi","Dirk Hovy"],"categories":[],"content":"","date":1628208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628208000,"objectID":"e222ff2d16d7fe8780871ce179aab3b6","permalink":"https://federicobianchi.io/publication/clip-italian/","publishdate":"2021-05-06T01:41:26+01:00","relpermalink":"/publication/clip-italian/","section":"publication","summary":"There are some issues with current research trends in NLP that can hamper the free development of scientific research. We identify five of particular concern: 1) the early adoption of methods without sufficient understanding or analysis; 2) the preference for computational methods regardless of risks associated with their limitations; 3) the resulting bias in the papers we publish; 4) the impossibility of re-running some experiments due to their cost; 5) the dangers of unexplainable methods.  If these issues are not addressed, we risk a loss of reproducibility, reputability, and subsequently public trust in our field. In this position paper, we outline each of these points and suggest ways forward.","tags":["Position Paper","Issues","NLP"],"title":"On the Gap between Adoption and Understanding in NLP","type":"publication"},{"authors":["Federico Bianchi","Silvia Terragni","Dirk Hovy"],"categories":[],"content":"","date":1628208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628208000,"objectID":"bfb2685f5e8a1acfedd73e05511df232","permalink":"https://federicobianchi.io/publication/contextualized-improve-topic-models-coherence/","publishdate":"2021-05-06T01:41:26+01:00","relpermalink":"/publication/contextualized-improve-topic-models-coherence/","section":"publication","summary":"Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized BERT representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-word topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models.","tags":["Topic Modeling","Coherence","NLP"],"title":"Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence","type":"publication"},{"authors":["Giovanni Cassani","Federico Bianchi","Marco Marelli"],"categories":null,"content":"","date":1617321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617321600,"objectID":"866bd8f2e6f4c7c9901bf44fe269e821","permalink":"https://federicobianchi.io/publication/diachronic-language-acquistion/","publishdate":"2021-04-02T00:00:00Z","relpermalink":"/publication/diachronic-language-acquistion/","section":"publication","summary":"In this study, we use temporally aligned word embeddings and a large diachronic corpus of English to quantify language change in a data‐driven, scalable way, which is grounded in language use.","tags":["NLP","Meaning","Linguistics","Embeddings","Semantic Change"],"title":"Words with Consistent Diachronic Usage Patterns are Learned Earlier: A Computational Analysis Using Temporally Aligned Word Embeddings.","type":"publication"},{"authors":["Federico Bianchi","Adriano Macarone"],"categories":null,"content":"","date":1616504400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616504400,"objectID":"96104981d75926385caf4dfaeb23d788","permalink":"https://federicobianchi.io/talk/talk-deep-learning-for-quantum-problems/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-deep-learning-for-quantum-problems/","section":"event","summary":"We introduce the main components present in deep learning architecture and the possible applications in quantum physics.","tags":[],"title":"Talk: Deep Learning for Quantum Problems","type":"event"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1616504400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616504400,"objectID":"08bf1e083c062f3d6aeb744275bc2362","permalink":"https://federicobianchi.io/talk/talk-fantastic-embeddings-and-how-to-align-them/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-fantastic-embeddings-and-how-to-align-them/","section":"event","summary":"In this talk I present our paper on aligning product embeddings that come from multiple shops. We use techniques from machine translation to provide an effective method for alignment.","tags":[],"title":"Talk: Fantastic Embeddings and How to Align Them","type":"event"},{"authors":["Federico Bianchi","Ciro Greco","Jacopo Tagliabue"],"categories":null,"content":"","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"d63fa74bda8dcedf3e9101f5944a969d","permalink":"https://federicobianchi.io/publication/language-in-a-search-box/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/publication/language-in-a-search-box/","section":"publication","summary":"We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines.","tags":["NLP","Meaning","Linguistics","BERT","Embeddings","Language Models"],"title":"Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction","type":"publication"},{"authors":["Federico Bianchi","Jacopo Tagliabue","Bingqing Yu"],"categories":null,"content":"","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"b1af67826fd1fd6d9e377d57b8d0cb82","permalink":"https://federicobianchi.io/publication/query2prod2vec/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/publication/query2prod2vec/","section":"publication","summary":"We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings: in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation: our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of data efficiency for product search outside of retail giants, and highlight how Query2Prod2Vec fits with practical constraints faced by most practitioners.","tags":["NLP","Meaning","Linguistics","BERT","Embeddings","Language Models","eCommerce"],"title":"Query2Prod2Vec: Grounded Word Embeddings for eCommerce","type":"publication"},{"authors":["Tommaso Fornaciari","Federico Bianchi","Massimo Poesio","Dirk Hovy"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"18d710d1bd07ff373dffa9c27d29a89b","permalink":"https://federicobianchi.io/publication/bertective/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/bertective/","section":"publication","summary":"Spotting a lie is challenging but has an enormous potential impact on security as well as private and public safety. Several NLP methods have been proposed to classify texts as truthful or deceptive. In most cases, however, the target texts' preceding context is not considered. This is a severe limitation, as any communication takes place in context, not in a vacuum, and context can help to detect deception. We study a corpus of Italian dialogues containing deceptive statements and implement deep neural models that incorporate various linguistic contexts. We establish a new state-of-the-art identifying deception and find that not all context is equally useful to the task. Only the texts closest to the target, if from the same speaker (rather than questions by an interlocutor), boost performance. We also find that the semantic information in language models such as BERT contributes to the performance. However, BERT alone does not capture the implicit knowledge of deception cues: its contribution is conditional on the concurrent use of attention to learn cues from BERT's representations.","tags":[],"title":"BERTective: Language Models and Contextual Information for Deception Detection","type":"publication"},{"authors":["Federico Bianchi","Silvia Terragni","Dirk Hovy","Debora Nozza","Elisabetta Fersini"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"1aea40578aaeb79054905662a2c0203a","permalink":"https://federicobianchi.io/publication/cross-lingual-contextualized-topic-models-for-zero-shot/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/cross-lingual-contextualized-topic-models-for-zero-shot/","section":"publication","summary":"We introduce a novel topic modeling method that can make use of contextulized embeddings (e.g., BERT) to do zero-shot cross-lingual topic modeling.","tags":["NLP","Topic Modeling","BERT","Language Models"],"title":"Cross-lingual Contextualized Topic Models with Zero-shot Learning","type":"publication"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1613566800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613566800,"objectID":"f962e0e4405e52f2de6ccd416d10f4b4","permalink":"https://federicobianchi.io/talk/talk-learning-jax-for-great-good/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-learning-jax-for-great-good/","section":"event","summary":"I gave a tutorial on JAX,  the new google framework for deep learning. I have described how to compute simple derivatives and finished describing some applications in NLP.","tags":[],"title":"Talk: Learning JAX for Great Good","type":"event"},{"authors":["Federico Bianchi","Debora Nozza","Dirk Hovy"],"categories":null,"content":"","date":1612569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612569600,"objectID":"60525cb6479816ccd24e6db1cfb9a0b1","permalink":"https://federicobianchi.io/publication/feel-it/","publishdate":"2021-02-06T00:00:00Z","relpermalink":"/publication/feel-it/","section":"publication","summary":"Sentiment analysis is a common task to understand people's reactions online. Still, we often need more nuanced information: is the post negative because the user is angry or because they are sad? An abundance of approaches has been introduced for tackling both tasks. However, at least for Italian, they all treat only one of the tasks at a time. We introduce FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions: anger, fear, joy, sadness. By collapsing them, we can also do sentiment analysis. We evaluate our corpus on benchmark datasets for both emotion and sentiment classification,  obtaining competitive results. We release an open-source Python library, so researchers can use a model trained on FEEL-IT for inferring both sentiments and emotions from Italian text.","tags":["NLP","Sentiment Analysis","BERT","Embeddings","Language Models"],"title":"FEEL-IT: Emotion and Sentiment Classification for the Italian Language","type":"publication"},{"authors":["Monireh Ebrahimi","Aaron Eberhart","Federico Bianchi","Pascal Hitzler"],"categories":null,"content":"","date":1612569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612569600,"objectID":"4164406b0bb8bbc9dfdbd1d672bb4f81","permalink":"https://federicobianchi.io/publication/towards-neuro-symbolic/","publishdate":"2021-02-06T00:00:00Z","relpermalink":"/publication/towards-neuro-symbolic/","section":"publication","summary":"Symbolic knowledge representation and reasoning and deep learning are fundamentally different approaches to artificial intelligence with complementary capabilities. The former are transparent and data-efficient, but they are sensitive to noise and cannot be applied to non-symbolic domains where the data is ambiguous. The latter can learn complex tasks from examples, are robust to noise, but are black boxes; require large amounts of –not necessarily easily obtained– data, and are slow to learn and prone to adversarial examples. Either paradigm excels at certain types of problems where the other paradigm performs poorly. In order to develop stronger AI systems, integrated neuro-symbolic systems that combine artificial neural networks and symbolic reasoning are being sought. In this context, one of the fundamental open problems is how to perform logic-based deductive reasoning over knowledge bases by means of trainable artificial neural networks. This paper provides a brief summary of the authors’ recent efforts to bridge the neural and symbolic divide in the context of deep deductive reasoners. Throughout the paper we will discuss strengths and limitations of models in term of accuracy, scalability, transferability, generalizabiliy, speed, and interpretability, and finally, will talk about possible modifications to enhance desirable capabilities. More specifically, in terms of architectures, we are looking at Memory-augmented networks, Logic Tensor Networks, and compositions of LSTM models to explore their capabilities and limitations in conducting deductive reasoning. We are applying these models on Resource Description Framework (RDF), first-order logic, and the description logic EL+ respectively.","tags":[],"title":"Towards bridging the neuro-symbolic gap: deep deductive reasoners.","type":"publication"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1602075600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602075600,"objectID":"1d1425392e64fd4f405da4d73da5d418","permalink":"https://federicobianchi.io/talk/talk-vector-space-alignment-for-semantic-change-analysis/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-vector-space-alignment-for-semantic-change-analysis/","section":"event","summary":"The use of compass aligned embeddings for semantic change analysis.","tags":[],"title":"Talk: Vector Space Alignment for Semantic Change Analysis","type":"event"},{"authors":["Federico Bianchi","Jacopo Tagliabue","Bingqing Yu","Luca Bigon","Ciro Greco"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"f8fd8b57e241733aaf36973be97744b2","permalink":"https://federicobianchi.io/publication/fantastic-embeddings/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/fantastic-embeddings/","section":"publication","summary":"In this paper we work on aligning product embeddings that come from different shops. We use techniques from machine translation to provide an effective method for alignment.","tags":["NLP","Ecommerce","Embeddings","Language Models"],"title":"Fantastic Embeddings and How to Align Them: Zero-Shot Inference in a Multi-Shop Scenario","type":"publication"},{"authors":["Valerio Di Carlo","Federico Bianchi","Matteo Palmonari"],"categories":null,"content":"","date":1562025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562025600,"objectID":"9614505ad4e0bea922311fe7aa99eb1c","permalink":"https://federicobianchi.io/publication/training-temporal-compass/","publishdate":"2019-07-02T00:00:00Z","relpermalink":"/publication/training-temporal-compass/","section":"publication","summary":"We introduce a novel model for word embedding alignment and test it on temporal word embeddings obtaining SOTA results.","tags":["NLP","Meaning","Temporal Word Embeddings","Embeddings"],"title":" Training Temporal Word Embeddings with a Compass","type":"publication"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1551445200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551445200,"objectID":"b820420e29d21b112d912bf43c8769fc","permalink":"https://federicobianchi.io/talk/talk-on-the-capabilities-of-logic-tensor-networks-for-deductive-reasoning/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/talk/talk-on-the-capabilities-of-logic-tensor-networks-for-deductive-reasoning/","section":"event","summary":"I presented our work on Logic Tensor Networks, where we explore its reasoning capabilities.","tags":[],"title":"Talk: On the Capabilities of Logic Tensor Networks for Deductive Reasoning","type":"event"},{"authors":["Federico Bianchi","Matteo Palmonari","Debora Nozza"],"categories":null,"content":"","date":1530489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530489600,"objectID":"3b401328c412d7509d701786182012cd","permalink":"https://federicobianchi.io/publication/towards-encoding-time/","publishdate":"2018-07-02T00:00:00Z","relpermalink":"/publication/towards-encoding-time/","section":"publication","summary":"Knowledge Graphs (KG) are widely used abstractions to represent entity-centric knowledge. Approaches to embed entities, entity types and relations represented in the graph into vector spaces - often referred to as KG embeddings - have become increasingly popular for their ability to capture the similarity between entities and support other reasoning tasks. However, representation of time has received little attention in these approaches. In this work, we make a first step to encode time into vector-based entity representations using a text-based KG embedding model named Typed Entity Embeddings (TEEs). In TEEs, each entity is represented by a vector that represents the entity and its type, which is learned from entity mentions found in a text corpus. Inspired by evidence from cognitive sciences and application-oriented concerns, we propose an approach to encode representations of years into TEEs by aggregating the representations of the entities that occur in event-based descriptions of the years. These representations are used to define two time-aware similarity measures to control the implicit effect of time on entity similarity. Experimental results show that the linear order of years obtained using our model is highly correlated with natural time flow and the effectiveness of the time-aware similarity measure proposed to flatten the time effect on entity similarity","tags":["Semantic Web","NLP","KG","Embeddings"],"title":"Towards Encoding Time in text-based Entity Embeddings","type":"publication"},{"authors":null,"categories":null,"content":"Add your privacy policy here and set draft: false to publish it. Otherwise, delete this file if you don\u0026rsquo;t need it.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://federicobianchi.io/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"Add your privacy policy here and set draft: false to publish it. Otherwise, delete this file if you don\u0026rsquo;t need it.","tags":null,"title":"Privacy Policy","type":"page"},{"authors":["Federico Bianchi","Matteo Palmonari","Marco Cremaschi","Elisabetta Fersini"],"categories":null,"content":"","date":1498953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498953600,"objectID":"915960b541cbc7136bb0c682f26ac312","permalink":"https://federicobianchi.io/publication/actively-learning-to-rank/","publishdate":"2017-07-02T00:00:00Z","relpermalink":"/publication/actively-learning-to-rank/","section":"publication","summary":"Knowledge Graphs (KG) represent a large amount of Semantic Associations (SAs), i.e., chains of relations that may reveal interesting and unknown connections between different types of entities. Applications for the contextual exploration of KGs help users explore information extracted from a KG, including SAs, while they are reading an input text. Because of the large number of SAs that can be extracted from a text, a first challenge in these applications is to effectively determine which SAs are most interesting to the users, defining a suitable ranking function over SAs. However, since different users may have different interests, an additional challenge is to personalize this ranking function to match individual users’ preferences. In this paper we introduce a novel active learning to rank model to let a user rate small samples of SAs, which are used to iteratively learn a personalized ranking function. Experiments conducted with two data sets show that the approach is able to improve the quality of the ranking function with a limited number of user interactions.","tags":["Semantic Web","NLP","KG"],"title":"Actively Learning to Rank Semantic Associations for Personalized Contextual Exploration of Knowledge Graphs","type":"publication"}]
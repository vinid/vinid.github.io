[{"authors":null,"categories":null,"content":"Hello! I am Federico Bianchi, a post-doctoral researcher at Bocconi University (with Prof. Dirk Hovy), Milan, Italy.\nYou can find my work in the main track of different AI venues (e.g., NAACL, EACL, AAAI, ACL, ISWC, RecSys) and Q1 Journals (e.g., Cognitive Science, Nature PJQI, SWJ) and a few things have also been featured in press or in company media outlets (see, 1, 2, 3, 4).\nI have different interests that range from neuro-symbolic learning and reasoning to natural language processing and from quantum physics to deep learning for the e-commerce.\nI have also worked/collaborated with companies: I have collaborated as a Data Scientist for Instal LLC and I have an ongoing research collaboration with people at Coveo on e-commerce and recommendation research.\nI have given talks in different places for both Universities (e.g., Edinburgh University, Sussex University, Tilburg University) and Companies (Coveo Labs, LightOn AI) and tutorials at major AI Conferences (IJCAI, IJCLR).\n","date":1617321600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617321600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello! I am Federico Bianchi, a post-doctoral researcher at Bocconi University (with Prof. Dirk Hovy), Milan, Italy.\nYou can find my work in the main track of different AI venues (e.","tags":null,"title":"Federico Bianchi","type":"authors"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1633611600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633611600,"objectID":"749fef9e9f36cd460aa29556ec288875","permalink":"https://federicobianchi.io/talk/talk-how-to-train-your-clip/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-how-to-train-your-clip/","section":"event","summary":"How to train a Contrastive Language-Image Pre-training for the Italian Language","tags":[],"title":"Talk: How to Train Your CLIP","type":"event"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1633266000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633266000,"objectID":"d18e98d594cef8793bd7277fd4a4a3c1","permalink":"https://federicobianchi.io/talk/talk-natural-language-processing-to-better-understand-language-and-vice-versa/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/talk/talk-natural-language-processing-to-better-understand-language-and-vice-versa/","section":"event","summary":"We show how NLP can help us understand language aspects like grounding, but also discuss several cognitively-informed methods that can be directly applied to more practical tasks.","tags":[],"title":"Talk: Natural Language Processing to Better Understand Language ‚Äì and Vice Versa","type":"event"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1631797200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631797200,"objectID":"befd1ecd4108bd765df01deda7fda6df","permalink":"https://federicobianchi.io/talk/talk-contrastive-language-image-pre-training-for-the-italian-language/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-contrastive-language-image-pre-training-for-the-italian-language/","section":"event","summary":"How to train a Contrastive Language-Image Pre-training for the Italian Language","tags":[],"title":"Talk: Contrastive Language-Image Pre-training for the Italian Language","type":"event"},{"authors":["Federico Bianchi","Giuseppe Attanasio","Raphael Pisoni","Silvia Terragni","Gabriele Sarti","Sri Lakshmi"],"categories":[],"content":"","date":1629590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629590400,"objectID":"998aa0368cc2996b98a9ec761feeb19c","permalink":"https://federicobianchi.io/publication/gap-between-understanding-adoption/","publishdate":"2021-08-22T01:41:26+01:00","relpermalink":"/publication/gap-between-understanding-adoption/","section":"publication","summary":"CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal model that jointly learns representations of images and texts. The model is trained on a massive amount of English data and shows impressive performance on zero-shot classification tasks. Training the same model on a different language is not trivial, since data in other languages might be not enough and the model needs high-quality translations of the texts to guarantee a good performance. In this paper, we present the first CLIP model for the Italian Language (CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image retrieval and zero-shot classification.","tags":["Multimodal","Vision","NLP"],"title":"Contrastive Language-Image Pre-training for the Italian Language","type":"publication"},{"authors":["Federico Bianchi","Silvia Terragni","Dirk Hovy"],"categories":[],"content":"","date":1628208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628208000,"objectID":"bfb2685f5e8a1acfedd73e05511df232","permalink":"https://federicobianchi.io/publication/contextualized-improve-topic-models-coherence/","publishdate":"2021-05-06T01:41:26+01:00","relpermalink":"/publication/contextualized-improve-topic-models-coherence/","section":"publication","summary":"Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized BERT representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-word topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models.","tags":["Topic Modeling","Coherence","NLP"],"title":"Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence","type":"publication"},{"authors":["Giovanni Cassani","Federico Bianchi","Marco Marelli"],"categories":null,"content":"","date":1617321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617321600,"objectID":"866bd8f2e6f4c7c9901bf44fe269e821","permalink":"https://federicobianchi.io/publication/diachronic-language-acquistion/","publishdate":"2021-04-02T00:00:00Z","relpermalink":"/publication/diachronic-language-acquistion/","section":"publication","summary":"In this study, we use temporally aligned word embeddings and a large diachronic corpus of English to quantify language change in a data‚Äêdriven, scalable way, which is grounded in language use.","tags":["NLP","Meaning","Linguistics","Embeddings","Semantic Change"],"title":"Words with Consistent Diachronic Usage Patterns are Learned Earlier: A Computational Analysis Using Temporally Aligned Word Embeddings.","type":"publication"},{"authors":["Federico Bianchi","Adriano Macarone"],"categories":null,"content":"","date":1616504400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616504400,"objectID":"96104981d75926385caf4dfaeb23d788","permalink":"https://federicobianchi.io/talk/talk-deep-learning-for-quantum-problems/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-deep-learning-for-quantum-problems/","section":"event","summary":"We introduce the main components present in deep learning architecture and the possible applications in quantum physics.","tags":[],"title":"Talk: Deep Learning for Quantum Problems","type":"event"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1616504400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616504400,"objectID":"08bf1e083c062f3d6aeb744275bc2362","permalink":"https://federicobianchi.io/talk/talk-fantastic-embeddings-and-how-to-align-them/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-fantastic-embeddings-and-how-to-align-them/","section":"event","summary":"In this talk I present our paper on aligning product embeddings that come from multiple shops. We use techniques from machine translation to provide an effective method for alignment.","tags":[],"title":"Talk: Fantastic Embeddings and How to Align Them","type":"event"},{"authors":["Federico Bianchi","Ciro Greco","Jacopo Tagliabue"],"categories":null,"content":"","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"d63fa74bda8dcedf3e9101f5944a969d","permalink":"https://federicobianchi.io/publication/language-in-a-search-box/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/publication/language-in-a-search-box/","section":"publication","summary":"We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines.","tags":["NLP","Meaning","Linguistics","BERT","Embeddings","Language Models"],"title":"Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction","type":"publication"},{"authors":["Federico Bianchi","Jacopo Tagliabue","Bingqing Yu"],"categories":null,"content":"","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"b1af67826fd1fd6d9e377d57b8d0cb82","permalink":"https://federicobianchi.io/publication/query2prod2vec/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/publication/query2prod2vec/","section":"publication","summary":"We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings: in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation: our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of data efficiency for product search outside of retail giants, and highlight how Query2Prod2Vec fits with practical constraints faced by most practitioners.","tags":["NLP","Meaning","Linguistics","BERT","Embeddings","Language Models","eCommerce"],"title":"Query2Prod2Vec: Grounded Word Embeddings for eCommerce","type":"publication"},{"authors":["Tommaso Fornaciari","Federico Bianchi","Massimo Poesio","Dirk Hovy"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"18d710d1bd07ff373dffa9c27d29a89b","permalink":"https://federicobianchi.io/publication/bertective/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/bertective/","section":"publication","summary":"Spotting a lie is challenging but has an enormous potential impact on security as well as private and public safety. Several NLP methods have been proposed to classify texts as truthful or deceptive. In most cases, however, the target texts' preceding context is not considered. This is a severe limitation, as any communication takes place in context, not in a vacuum, and context can help to detect deception. We study a corpus of Italian dialogues containing deceptive statements and implement deep neural models that incorporate various linguistic contexts. We establish a new state-of-the-art identifying deception and find that not all context is equally useful to the task. Only the texts closest to the target, if from the same speaker (rather than questions by an interlocutor), boost performance. We also find that the semantic information in language models such as BERT contributes to the performance. However, BERT alone does not capture the implicit knowledge of deception cues: its contribution is conditional on the concurrent use of attention to learn cues from BERT's representations.","tags":[],"title":"BERTective: Language Models and Contextual Information for Deception Detection","type":"publication"},{"authors":["Federico Bianchi","Silvia Terragni","Dirk Hovy","Debora Nozza","Elisabetta Fersini"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"1aea40578aaeb79054905662a2c0203a","permalink":"https://federicobianchi.io/publication/cross-lingual-contextualized-topic-models-for-zero-shot/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/cross-lingual-contextualized-topic-models-for-zero-shot/","section":"publication","summary":"We introduce a novel topic modeling method that can make use of contextulized embeddings (e.g., BERT) to do zero-shot cross-lingual topic modeling.","tags":["NLP","Topic Modeling","BERT","Language Models"],"title":"Cross-lingual Contextualized Topic Models with Zero-shot Learning","type":"publication"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1613566800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613566800,"objectID":"f962e0e4405e52f2de6ccd416d10f4b4","permalink":"https://federicobianchi.io/talk/talk-learning-jax-for-great-good/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-learning-jax-for-great-good/","section":"event","summary":"I gave a tutorial on JAX,  the new google framework for deep learning. I have described how to compute simple derivatives and finished describing some applications in NLP.","tags":[],"title":"Talk: Learning JAX for Great Good","type":"event"},{"authors":["Federico Bianchi","Debora Nozza","Dirk Hovy"],"categories":null,"content":"","date":1612569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612569600,"objectID":"60525cb6479816ccd24e6db1cfb9a0b1","permalink":"https://federicobianchi.io/publication/feel-it/","publishdate":"2021-02-06T00:00:00Z","relpermalink":"/publication/feel-it/","section":"publication","summary":"Sentiment analysis is a common task to understand people's reactions online. Still, we often need more nuanced information: is the post negative because the user is angry or because they are sad? An abundance of approaches has been introduced for tackling both tasks. However, at least for Italian, they all treat only one of the tasks at a time. We introduce FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions: anger, fear, joy, sadness. By collapsing them, we can also do sentiment analysis. We evaluate our corpus on benchmark datasets for both emotion and sentiment classification,  obtaining competitive results. We release an open-source Python library, so researchers can use a model trained on FEEL-IT for inferring both sentiments and emotions from Italian text.","tags":["NLP","Sentiment Analysis","BERT","Embeddings","Language Models"],"title":"FEEL-IT: Emotion and Sentiment Classification for the Italian Language","type":"publication"},{"authors":["Monireh Ebrahimi","Aaron Eberhart","Federico Bianchi","Pascal Hitzler"],"categories":null,"content":"","date":1612569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612569600,"objectID":"4164406b0bb8bbc9dfdbd1d672bb4f81","permalink":"https://federicobianchi.io/publication/towards-neuro-symbolic/","publishdate":"2021-02-06T00:00:00Z","relpermalink":"/publication/towards-neuro-symbolic/","section":"publication","summary":"Symbolic knowledge representation and reasoning and deep learning are fundamentally different approaches to artificial intelligence with complementary capabilities. The former are transparent and data-efficient, but they are sensitive to noise and cannot be applied to non-symbolic domains where the data is ambiguous. The latter can learn complex tasks from examples, are robust to noise, but are black boxes; require large amounts of ‚Äìnot necessarily easily obtained‚Äì data, and are slow to learn and prone to adversarial examples. Either paradigm excels at certain types of problems where the other paradigm performs poorly. In order to develop stronger AI systems, integrated neuro-symbolic systems that combine artificial neural networks and symbolic reasoning are being sought. In this context, one of the fundamental open problems is how to perform logic-based deductive reasoning over knowledge bases by means of trainable artificial neural networks. This paper provides a brief summary of the authors‚Äô recent efforts to bridge the neural and symbolic divide in the context of deep deductive reasoners. Throughout the paper we will discuss strengths and limitations of models in term of accuracy, scalability, transferability, generalizabiliy, speed, and interpretability, and finally, will talk about possible modifications to enhance desirable capabilities. More specifically, in terms of architectures, we are looking at Memory-augmented networks, Logic Tensor Networks, and compositions of LSTM models to explore their capabilities and limitations in conducting deductive reasoning. We are applying these models on Resource Description Framework (RDF), first-order logic, and the description logic EL+ respectively.","tags":[],"title":"Towards bridging the neuro-symbolic gap: deep deductive reasoners.","type":"publication"},{"authors":["Federico Bianchi","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://federicobianchi.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1602075600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602075600,"objectID":"1d1425392e64fd4f405da4d73da5d418","permalink":"https://federicobianchi.io/talk/talk-vector-space-alignment-for-semantic-change-analysis/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/talk-vector-space-alignment-for-semantic-change-analysis/","section":"event","summary":"The use of compass aligned embeddings for semantic change analysis.","tags":[],"title":"Talk: Vector Space Alignment for Semantic Change Analysis","type":"event"},{"authors":["Federico Bianchi","Jacopo Tagliabue","Bingqing Yu","Luca Bigon","Ciro Greco"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"f8fd8b57e241733aaf36973be97744b2","permalink":"https://federicobianchi.io/publication/fantastic-embeddings/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/fantastic-embeddings/","section":"publication","summary":"In this paper we work on aligning product embeddings that come from different shops. We use techniques from machine translation to provide an effective method for alignment.","tags":["NLP","Ecommerce","Embeddings","Language Models"],"title":"Fantastic Embeddings and How to Align Them: Zero-Shot Inference in a Multi-Shop Scenario","type":"publication"},{"authors":["Valerio Di Carlo","Federico Bianchi","Matteo Palmonari"],"categories":null,"content":"","date":1562025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562025600,"objectID":"9614505ad4e0bea922311fe7aa99eb1c","permalink":"https://federicobianchi.io/publication/training-temporal-compass/","publishdate":"2019-07-02T00:00:00Z","relpermalink":"/publication/training-temporal-compass/","section":"publication","summary":"We introduce a novel model for word embedding alignment and test it on temporal word embeddings obtaining SOTA results.","tags":["NLP","Meaning","Temporal Word Embeddings","Embeddings"],"title":" Training Temporal Word Embeddings with a Compass","type":"publication"},{"authors":["Federico Bianchi"],"categories":null,"content":"","date":1551445200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551445200,"objectID":"b820420e29d21b112d912bf43c8769fc","permalink":"https://federicobianchi.io/talk/talk-on-the-capabilities-of-logic-tensor-networks-for-deductive-reasoning/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/talk/talk-on-the-capabilities-of-logic-tensor-networks-for-deductive-reasoning/","section":"event","summary":"I presented our work on Logic Tensor Networks, where we explore its reasoning capabilities.","tags":[],"title":"Talk: On the Capabilities of Logic Tensor Networks for Deductive Reasoning","type":"event"},{"authors":["Federico Bianchi","Matteo Palmonari","Debora Nozza"],"categories":null,"content":"","date":1530489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530489600,"objectID":"3b401328c412d7509d701786182012cd","permalink":"https://federicobianchi.io/publication/towards-encoding-time/","publishdate":"2018-07-02T00:00:00Z","relpermalink":"/publication/towards-encoding-time/","section":"publication","summary":"Knowledge Graphs (KG) are widely used abstractions to represent entity-centric knowledge. Approaches to embed entities, entity types and relations represented in the graph into vector spaces - often referred to as KG embeddings - have become increasingly popular for their ability to capture the similarity between entities and support other reasoning tasks. However, representation of time has received little attention in these approaches. In this work, we make a first step to encode time into vector-based entity representations using a text-based KG embedding model named Typed Entity Embeddings (TEEs). In TEEs, each entity is represented by a vector that represents the entity and its type, which is learned from entity mentions found in a text corpus. Inspired by evidence from cognitive sciences and application-oriented concerns, we propose an approach to encode representations of years into TEEs by aggregating the representations of the entities that occur in event-based descriptions of the years. These representations are used to define two time-aware similarity measures to control the implicit effect of time on entity similarity. Experimental results show that the linear order of years obtained using our model is highly correlated with natural time flow and the effectiveness of the time-aware similarity measure proposed to flatten the time effect on entity similarity","tags":["Semantic Web","NLP","KG","Embeddings"],"title":"Towards Encoding Time in text-based Entity Embeddings","type":"publication"},{"authors":["Federico Bianchi","Matteo Palmonari","Marco Cremaschi","Elisabetta Fersini"],"categories":null,"content":"","date":1498953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498953600,"objectID":"915960b541cbc7136bb0c682f26ac312","permalink":"https://federicobianchi.io/publication/actively-learning-to-rank/","publishdate":"2017-07-02T00:00:00Z","relpermalink":"/publication/actively-learning-to-rank/","section":"publication","summary":"Knowledge Graphs (KG) represent a large amount of Semantic Associations (SAs), i.e., chains of relations that may reveal interesting and unknown connections between different types of entities. Applications for the contextual exploration of KGs help users explore information extracted from a KG, including SAs, while they are reading an input text. Because of the large number of SAs that can be extracted from a text, a first challenge in these applications is to effectively determine which SAs are most interesting to the users, defining a suitable ranking function over SAs. However, since different users may have different interests, an additional challenge is to personalize this ranking function to match individual users‚Äô preferences. In this paper we introduce a novel active learning to rank model to let a user rate small samples of SAs, which are used to iteratively learn a personalized ranking function. Experiments conducted with two data sets show that the approach is able to improve the quality of the ranking function with a limited number of user interactions.","tags":["Semantic Web","NLP","KG"],"title":"Actively Learning to Rank Semantic Associations for Personalized Contextual Exploration of Knowledge Graphs","type":"publication"}]